# Plan de Implementaci√≥n: Settings Din√°micos Completos

## üìä Resumen Ejecutivo

**Estado**: ‚úÖ IMPLEMENTACI√ìN COMPLETADA

**Logros**:
- ‚úÖ 22 settings totales (eliminados 2 obsoletos: scraper_urls, epg_sources)
- ‚úÖ 9 settings din√°micos (se aplican sin reiniciar)
- ‚úÖ 13 settings que requieren restart
- ‚úÖ Gesti√≥n profesional de URLs (ScraperURL y EPGSource)
- ‚úÖ APIs REST completas para gesti√≥n de fuentes
- ‚úÖ Servicios leen de tablas en lugar de settings

**Resultado**: Sistema completamente funcional con configuraci√≥n din√°mica y gesti√≥n profesional de URLs.

---

## üéØ Objetivo

Hacer que TODOS los settings sean reales y utilizables, con la capacidad de cambiar valores din√°micamente sin reiniciar el servidor cuando sea t√©cnicamente posible.

## üìã Alcance del Proyecto

### Parte 1: Settings Din√°micos (6 valores)
Hacer que estos settings se lean din√°micamente y se apliquen sin reiniciar:
1. ‚úÖ `scraper_update_interval` - YA IMPLEMENTADO
2. ‚úÖ `epg_update_interval` - YA IMPLEMENTADO
3. ‚úÖ `server_timezone` - YA IMPLEMENTADO
4. ‚è≥ `epg_cache_file` - Por implementar
5. ‚è≥ `acestream_timeout` - Por implementar
6. ‚è≥ `acestream_chunk_size` - Por implementar
7. ‚è≥ `acestream_empty_timeout` - Por implementar
8. ‚è≥ `acestream_no_response_timeout` - Por implementar
9. ‚è≥ `access_token_expire_minutes` - Por implementar

### Parte 2: Gesti√≥n Profesional de URLs
Reemplazar los settings de texto plano por gesti√≥n individual de URLs:
1. ‚è≥ Eliminar `scraper_urls` de Settings
2. ‚è≥ Eliminar `epg_sources` de Settings
3. ‚è≥ Crear API para gestionar ScraperURL (tabla ya existe)
4. ‚è≥ Crear API para gestionar EPGSource (tabla ya existe)
5. ‚è≥ Modificar servicios para leer de las tablas

### Parte 3: Documentaci√≥n
1. ‚è≥ Actualizar SETTINGS-DINAMICOS.md
2. ‚è≥ Actualizar API-REFERENCE.md
3. ‚è≥ Actualizar MEJORAS-IMPLEMENTADAS.md
4. ‚è≥ Crear gu√≠a de uso para gesti√≥n de URLs

---

## ‚úÖ FASE 1: APIs para Gesti√≥n de URLs - COMPLETADA

### Estado: ‚úÖ COMPLETADA
### Fecha de completaci√≥n: 24 de enero de 2026

### Objetivo
Crear endpoints profesionales para gestionar fuentes M3U y EPG individualmente.

### Archivos a Crear

#### 1.1. `app/api/scraper.py` - API de Scraper URLs
```python
# Endpoints:
GET    /api/scraper/sources          # Listar todas las fuentes
POST   /api/scraper/sources          # Agregar nueva fuente
PUT    /api/scraper/sources/{id}     # Actualizar fuente
DELETE /api/scraper/sources/{id}     # Eliminar fuente
```

**Funcionalidades**:
- Listar todas las URLs de scraper con su estado
- Agregar nuevas URLs individualmente
- Habilitar/deshabilitar URLs sin borrarlas
- Validar que no haya URLs duplicadas
- Mostrar estad√≠sticas (√∫ltima vez scrapeada, canales encontrados)

#### 1.2. `app/api/epg.py` - API de EPG Sources
```python
# Endpoints:
GET    /api/epg/sources              # Listar todas las fuentes
POST   /api/epg/sources              # Agregar nueva fuente
PUT    /api/epg/sources/{id}         # Actualizar fuente
DELETE /api/epg/sources/{id}         # Eliminar fuente
```

**Funcionalidades**:
- Listar todas las URLs de EPG con su estado
- Agregar nuevas URLs individualmente
- Habilitar/deshabilitar URLs sin borrarlas
- Validar que no haya URLs duplicadas
- Mostrar estad√≠sticas (√∫ltima actualizaci√≥n, programas encontrados)

### Archivos a Modificar

#### 1.3. `main.py`
```python
# Agregar imports:
from app.api import scraper
from app.api import epg

# Registrar routers:
app.include_router(scraper.router, prefix="/api", tags=["Scraper"])
app.include_router(epg.router, prefix="/api", tags=["EPG"])
```

### Pruebas

```bash
# Listar fuentes actuales
curl http://localhost:6880/api/scraper/sources -u "admin:Admin2024!Secure"
curl http://localhost:6880/api/epg/sources -u "admin:Admin2024!Secure"

# Agregar nueva fuente M3U
curl -X POST http://localhost:6880/api/scraper/sources \
  -u "admin:Admin2024!Secure" \
  -H "Content-Type: application/json" \
  -d '{"url":"https://nueva-fuente.com/lista.m3u","is_enabled":true}'

# Agregar nueva fuente EPG
curl -X POST http://localhost:6880/api/epg/sources \
  -u "admin:Admin2024!Secure" \
  -H "Content-Type: application/json" \
  -d '{"url":"https://nueva-fuente.com/epg.xml","is_enabled":true}'

# Deshabilitar una fuente sin borrarla
curl -X PUT http://localhost:6880/api/scraper/sources/1 \
  -u "admin:Admin2024!Secure" \
  -H "Content-Type: application/json" \
  -d '{"is_enabled":false}'

# Eliminar una fuente
curl -X DELETE http://localhost:6880/api/scraper/sources/2 \
  -u "admin:Admin2024!Secure"
```

---

## ‚úÖ FASE 2: Eliminar Settings Obsoletos - COMPLETADA

### Estado: ‚úÖ COMPLETADA
### Fecha de completaci√≥n: 24 de enero de 2026

### Objetivo
Eliminar `scraper_urls` y `epg_sources` de Settings ya que ahora se gestionan con las tablas.

### Archivos a Modificar

#### 2.1. `main.py` - Inicializaci√≥n de Settings
```python
# ELIMINAR estas l√≠neas:
Setting(key="scraper_urls", value=",".join(config.get_scraper_urls_list()), ...),
Setting(key="epg_sources", value=",".join(config.get_epg_sources_list()), ...),

# Resultado: 22 settings en lugar de 24
```

#### 2.2. Base de datos existente
```bash
# Si ya tienes settings creados, eliminarlos:
curl -X DELETE http://localhost:6880/api/settings/scraper_urls -u "admin:Admin2024!Secure"
curl -X DELETE http://localhost:6880/api/settings/epg_sources -u "admin:Admin2024!Secure"
```

### Resultado
- Settings pasa de 24 a 22 entradas
- Las URLs se gestionan exclusivamente desde las tablas ScraperURL y EPGSource

---

## ‚úÖ FASE 3: Hacer Din√°micos los 6 Settings Restantes - COMPLETADA

### Estado: ‚úÖ COMPLETADA
### Fecha de completaci√≥n: 24 de enero de 2026

### Objetivo
Modificar el c√≥digo para que estos 6 settings se lean din√°micamente.

### 3.1. `epg_cache_file` - Din√°mico

**Archivo**: `app/services/epg_service.py`

**Cambio**:
```python
# ANTES (l√≠nea ~156):
cache_file = self.config.epg_cache_file

# DESPU√âS:
from app.config import get_config
config = get_config()
cache_file = config.epg_cache_file
```

**D√≥nde aplicar**: En los m√©todos que guardan/cargan el cache EPG.

### 3.2. `acestream_timeout` - Din√°mico

**Archivo**: `app/services/aceproxy_service.py`

**Cambio**:
```python
# ANTES (l√≠nea ~65):
self.timeout = timeout  # Fijado al inicio

# DESPU√âS:
# Leer din√°micamente en cada petici√≥n
from app.config import get_config
config = get_config()
timeout = config.acestream_timeout
```

**D√≥nde aplicar**: En el m√©todo `check_stream_availability()` y otros que usan timeout.

### 3.3. `acestream_chunk_size` - Din√°mico

**Archivo**: `app/services/aiohttp_streaming_server.py`

**Cambio**:
```python
# Leer din√°micamente al crear nuevos streams
from app.config import get_config
config = get_config()
chunk_size = config.acestream_chunk_size
```

**Nota**: Los streams existentes mantienen su chunk_size, solo los nuevos usan el valor actualizado.

### 3.4. `acestream_empty_timeout` - Din√°mico

**Archivo**: `app/services/aiohttp_streaming_server.py`

**Cambio**: Similar a chunk_size, leer din√°micamente.

### 3.5. `acestream_no_response_timeout` - Din√°mico

**Archivo**: `app/services/aiohttp_streaming_server.py`

**Cambio**: Similar a chunk_size, leer din√°micamente.

### 3.6. `access_token_expire_minutes` - Din√°mico

**Archivo**: `app/utils/auth.py` (si existe generaci√≥n de tokens)

**Cambio**:
```python
# Leer din√°micamente al generar tokens
from app.config import get_config
config = get_config()
expire_minutes = config.access_token_expire_minutes
```

---

## ‚úÖ FASE 4: Modificar Servicios para Leer de Tablas - COMPLETADA

### Estado: ‚úÖ COMPLETADA
### Fecha de completaci√≥n: 24 de enero de 2026

### Objetivo
Hacer que los servicios lean las URLs desde las tablas en lugar de Settings.

### 4.1. Scraper Service

**Archivo**: `app/services/scraper_service.py`

**M√©todo**: `scrape_m3u_sources()`

**Cambio**:
```python
# ANTES:
# Lee de config (que lee de Settings o .env)
scraper_urls = config.get_scraper_urls_list()

# DESPU√âS:
# Lee directamente de la tabla ScraperURL
from app.models import ScraperURL
scraper_urls_objs = db.query(ScraperURL).filter(ScraperURL.is_enabled == True).all()
scraper_urls = [url.url for url in scraper_urls_objs]
```

### 4.2. EPG Service

**Archivo**: `app/services/epg_service.py`

**M√©todo**: `auto_update_loop()`

**Cambio**:
```python
# ANTES:
xmltv_sources = self.config.get_epg_sources_list()

# DESPU√âS:
from app.models import EPGSource
epg_sources_objs = self.db.query(EPGSource).filter(EPGSource.is_enabled == True).all()
xmltv_sources = [source.url for source in epg_sources_objs]
```

---

## üì¶ FASE 5: Documentaci√≥n

### 5.1. Actualizar `SETTINGS-DINAMICOS.md`

Agregar secciones:
- Lista completa de los 9 settings din√°micos
- Lista de los 13 settings que requieren restart
- Gu√≠a de uso de las nuevas APIs de URLs
- Ejemplos de uso completos

### 5.2. Actualizar `API-REFERENCE.md`

Agregar documentaci√≥n de:
- `GET /api/scraper/sources`
- `POST /api/scraper/sources`
- `PUT /api/scraper/sources/{id}`
- `DELETE /api/scraper/sources/{id}`
- `GET /api/epg/sources`
- `POST /api/epg/sources`
- `PUT /api/epg/sources/{id}`
- `DELETE /api/epg/sources/{id}`

### 5.3. Actualizar `MEJORAS-IMPLEMENTADAS.md`

Documentar todos los cambios realizados.

---

## üìä Resumen de Cambios

### Archivos Nuevos (2)
1. `app/api/scraper.py` - API de gesti√≥n de fuentes M3U
2. `app/api/epg.py` - API de gesti√≥n de fuentes EPG

### Archivos Modificados (6)
1. `main.py` - Registrar nuevos routers, eliminar 2 settings
2. `app/services/scraper_service.py` - Leer de tabla ScraperURL
3. `app/services/epg_service.py` - Leer de tabla EPGSource, epg_cache_file din√°mico
4. `app/services/aceproxy_service.py` - Timeouts din√°micos
5. `app/services/aiohttp_streaming_server.py` - Chunk size y timeouts din√°micos
6. `app/utils/auth.py` - Token expire din√°mico (si aplica)

### Documentaci√≥n Actualizada (3)
1. `SETTINGS-DINAMICOS.md`
2. `API-REFERENCE.md`
3. `MEJORAS-IMPLEMENTADAS.md`

---

## ‚úÖ Resultado Final

### Settings (22 total)

**Din√°micos (9)** - Se aplican sin reiniciar:
1. scraper_update_interval
2. epg_update_interval
3. server_timezone
4. epg_cache_file
5. acestream_timeout
6. acestream_chunk_size
7. acestream_empty_timeout
8. acestream_no_response_timeout
9. access_token_expire_minutes

**Requieren Restart (13)**:
1. server_host
2. server_port
3. server_debug
4. acestream_enabled
5. acestream_engine_host
6. acestream_engine_port
7. acestream_streaming_host
8. acestream_streaming_port
9. database_url
10. database_echo
11. database_pool_size
12. database_max_overflow
13. admin_username

### Gesti√≥n de URLs

**Scraper URLs**:
- Gesti√≥n individual desde `/api/scraper/sources`
- Agregar/eliminar/habilitar/deshabilitar URLs
- Sin l√≠mite de URLs
- Estad√≠sticas por URL

**EPG Sources**:
- Gesti√≥n individual desde `/api/epg/sources`
- Agregar/eliminar/habilitar/deshabilitar URLs
- Sin l√≠mite de URLs
- Estad√≠sticas por URL

---

## üöÄ Orden de Implementaci√≥n

1. ‚úÖ **FASE 1**: Crear APIs de gesti√≥n de URLs (scraper.py, epg.py) - COMPLETADA
2. ‚úÖ **FASE 2**: Eliminar settings obsoletos (scraper_urls, epg_sources) - COMPLETADA
3. ‚úÖ **FASE 3**: Hacer din√°micos los 6 settings restantes - COMPLETADA
4. ‚úÖ **FASE 4**: Modificar servicios para leer de tablas - COMPLETADA
5. ‚úÖ **FASE 5**: Actualizar documentaci√≥n completa - COMPLETADA
6. ‚úÖ **FASE 6**: Compilar, desplegar y probar - COMPLETADA
7. ‚úÖ **FASE 7**: Commit y push - COMPLETADA

---

**Fecha de creaci√≥n**: 24 de enero de 2026
**Fecha de completaci√≥n**: 24 de enero de 2026
**Estado**: ‚úÖ IMPLEMENTACI√ìN COMPLETADA AL 100%

**Commit**: `c7a2be2` - "Settings Din√°micos Completos y Gesti√≥n Profesional de URLs"
